---
title: "Backend Design"
---

Backend Architecture of Decentralized Exchanges on EVM Chains
TL;DR for otX.markets (production): Subgraph → Goldsky Pipeline → Supabase (mirror.*) → public views/derived tables → Frontend Realtime. Backend only prepares unsigned txs and maintains aggregates; FE reads Supabase.
Introduction
Decentralized exchanges (DEXes) like Uniswap and Whales.market rely on smart contracts on EVM-compatible blockchains to execute trades trustlessly. However, delivering a smooth user experience (real-time order updates, price displays, transaction status) requires a robust off-chain backend infrastructure layered on top of the blockchain. This backend includes indexers, RPC nodes, databases, caching layers, and real-time APIs that work in tandem to track on-chain activity and serve data to users efficiently. Below we explore how modern EVM-based DEXes implement these components – from indexing blockchain events and monitoring user transactions, to maintaining order books and syncing data between frontends and backends – often using tools like The Graph, custom indexer services, WebSocket APIs, and pub-sub messaging for real-time updates.
Indexers, RPC Nodes, and Databases for On-Chain Data
Blockchain Indexers are services that continuously listen to blockchain events and organize that data into a database for quick querying
subquery.medium.com
. In simple terms, an indexer connects to a blockchain node (via RPC or WebSocket) and extracts on-chain data (transactions, logs, etc.), then stores it in a more query-friendly form (usually an SQL database). This is essential because directly querying an Ethereum node for complex data (like all swaps in a DEX or a user’s trading history) is inefficient. The indexer works by filtering relevant events or state changes from blocks and building structured records in a database. On Ethereum, a basic approach is to use the JSON-RPC eth_getLogs method to retrieve contract event logs filtered by address and event signature
rocknblock.io
. Logs are emitted by smart contracts (for example, a Uniswap pool contract emitting a Swap or Mint event) precisely to make off-chain indexing easier. The indexer fetches these logs (or subscribes to them in real-time) and uses them to construct higher-level data views. If simple log queries are insufficient (e.g. to get additional context like timestamps or related state), more advanced techniques like retrieving block receipts or even execution traces can be used
rocknblock.io
rocknblock.io
, but the core idea remains: move on-chain data into an off-chain database for fast, flexible queries. Modern DEX projects commonly use indexer frameworks such as The Graph or SubQuery to streamline this process. For example, Uniswap leverages The Graph’s hosted service by deploying Subgraphs – essentially predefined indexer scripts – for each major version of the protocol
docs.uniswap.org
. These subgraphs listen to Uniswap factory and pool contracts for events like pool creation, swaps, liquidity additions, etc., and populate a PostgreSQL database with structured entities (accounts, trades, pool stats, etc.). The Uniswap v2/v3 analytics sites (Uniswap Info) then query this indexed data via GraphQL. The benefits are clear: the subgraph captures all exchange activity – token swap events, liquidity positions, daily volumes, price updates – and organizes it for easy retrieval
thegraph.com
. As The Graph team noted, “The Uniswap subgraph captures all exchange data including token purchases, liquidity creation, daily aggregates, historical volume, price changes and user activity.”
thegraph.com
 This rich indexed dataset enables features like historical charts, top trader leaderboards, and ROI calculations that would be impractical to assemble with on-chain calls alone
thegraph.com
thegraph.com
. Other DEXes use similar approaches. Whales.market, being a multi-chain OTC DEX, likely employs a combination of custom indexers or third-party services (they list SubQuery as a partner) to track events on each supported chain (Ethereum, Solana, Base, Manta, etc.). In practice, an indexer component will connect to an RPC node on each chain (e.g. an Ethereum JSON-RPC endpoint, a Solana WebSocket endpoint) and monitor the Whales.market smart contracts for OrderCreated, OrderFilled, OrderCanceled, and other relevant events. Detected events and state changes are then written into a backend database (for example, recording a new order in an “orders” table, updating balances in a “user_accounts” table, etc.). Using a database (often PostgreSQL for relational data) allows the DEX backend to efficiently query the latest state (open orders, order book snapshots, trade history) without repeatedly scanning the blockchain. The indexer, RPC node, and database thus work in tandem: the RPC node provides raw blockchain data, the indexer extracts and transforms it, and the database stores it for fast access. Notably, some projects run custom indexer services for greater control or performance. For instance, the decentralized exchange dYdX (v4) built a bespoke indexer on its Cosmos-based chain, but the concept translates to EVM as well. Their indexer is an open-source, read-only service optimized for high-frequency queries from web and mobile clients
docs.dydx.exchange
. Rather than hitting full blockchain nodes for every frontend query (which would be slow and resource-intensive), the indexer maintains its own copy of on-chain state in a conventional database and serves data via Web2-friendly APIs. In the dYdX architecture, on-chain events are streamed from the node into the indexer, which then applies those events to a Postgres database and prepares them for querying
docs.dydx.exchange
. The result is that clients can get light-speed responses for complex queries (like “open orders in the order book” or “user’s positions”) by querying the indexer’s database, instead of waiting on blockchain RPC calls. In short, indexers bridge the blockchain and database worlds, ensuring DEX applications have up-to-date data readily available.
Real-Time Transaction Monitoring and Frontend Updates
Tracking blockchain data is not just about eventual consistency in a database – DEX users also expect real-time feedback, especially for their own transactions. When a user submits a swap or order via the frontend, the system needs to reflect its status (pending, confirmed, or failed) nearly instantaneously to enhance user experience and trust. DEX backends achieve this by monitoring user transactions in the mempool and new blocks, often using WebSocket connections or dedicated notification services. As soon as a user signs and broadcasts a transaction (e.g. an order placement on Whales.market or a token swap on Uniswap), the frontend typically marks it as “pending”. The application obtains the transaction hash immediately upon submission, which serves as a unique identifier to track it
ethereum.stackexchange.com
. From this point, the DEX frontend or backend enters a monitoring loop: it will watch the blockchain for that hash to be mined into a block. The simplest approach is polling – e.g. periodically calling eth_getTransactionReceipt(txHash) via the RPC node. When the transaction is not yet mined, the receipt will be empty; once the transaction is included in a block, the receipt becomes available and includes a status field indicating success or failure
ethereum.stackexchange.com
. A status of 1 means the transaction was successful (confirmed on-chain without revert), whereas 0 indicates it ran out of gas or hit a require() failure (failed). This polling can be done directly in the frontend (using libraries like Ethers.js which provide a provider.waitForTransaction or similar utility) or handled by a backend service that then notifies the frontend. For a more responsive solution, many DEXes use WebSocket subscriptions to get immediate alerts of relevant blockchain events. Ethereum nodes (and providers like Infura/Alchemy) support WebSocket endpoints where clients can subscribe to new blocks or even pending transactions. A DEX backend might subscribe to new blocks and check if the user’s transaction hash is in that block’s list of transactions. The moment a block containing the transaction is mined, the backend can push a notification (via WebSocket or server-sent events) to the frontend to update the status from “pending” to “confirmed/failed”. In cases where a transaction is dropped or replaced (e.g. user re-submits with higher gas), the backend can detect that the original hash is no longer in the mempool and update the UI accordingly (one challenge noted by Uniswap users is that if a tx is replaced, the UI needs to catch that and not indefinitely show it as pending
ethereum.stackexchange.com
). Some projects integrate specialized infrastructure for this purpose. For example, Blocknative and Alchemy Notify offer webhook or WebSocket notifications for transaction lifecycle events (submitted, mined, dropped). However, even without these, a combination of client-side and server-side monitoring does the job. The key is that the frontend is kept in sync with on-chain reality within a few seconds: users see feedback like “Transaction submitted... ⏳”, then “✅ Transaction confirmed” (or an error if failed) with minimal delay. This near-real-time reflection is crucial on DEX platforms, where users often execute time-sensitive trades. In the context of Whales.market, which spans multiple chains, transaction monitoring might be handled per chain. For EVM-based transactions (Ethereum, Base, etc.), the approach above applies – possibly with a unified backend service that listens to each connected chain’s new blocks and filters for transactions relevant to Whales.market contracts or user addresses of interest. For Solana transactions (since Whales.market also supports Solana), the mechanism would use Solana’s onSignature subscription or polling their APIs to detect confirmation. Regardless of chain, the result is the same: the backend feeds the frontend with status updates. Many DEX frontends open a WebSocket connection to their backend specifically for event updates, so the backend can broadcast messages like “order 0xABC filled” or “tx 0x123 confirmed at block XYZ” to all relevant clients instantly.
Maintaining Order Books and Price Data
Different DEX designs handle order and price data differently. Automated Market Maker (AMM) exchanges like Uniswap do not maintain a traditional order book – prices are determined by on-chain liquidity pool formulas. In contrast, order-driven exchanges or OTC platforms (like Whales.market’s pre-market and points trading) do present an order book of buy/sell offers. In both cases, the backend infrastructure ensures that price and market data remain up-to-date and consistent between backend and frontend. For Uniswap and other AMMs, “price data” usually means the current pool exchange rate and historical price trends. The current price of a token pair can be derived from on-chain state (pool reserves) at any time. Frontends often fetch this on demand by calling the pool contract’s state (e.g. reserve balances) via an RPC call or using a SDK formula. However, for displaying charts, 24h price changes, volumes, etc., the indexer plays a big role. The Uniswap subgraph, for instance, continuously aggregates trade events to calculate metrics like daily volume, liquidity, and price candles for each pool
thegraph.com
. This data is stored in the subgraph’s database and queried by the frontend’s analytics components. When a new swap event comes in, the indexer updates the pool’s running totals and perhaps computes a new price point for the latest block. Thus the backend maintains a canonical record of prices over time. The frontend queries this periodically or on page load to render, say, the price chart and the current price. To stay in sync in real-time, the frontend might also listen for new swap events. It could either subscribe to The Graph’s GraphQL subscriptions (if available) or open a direct WebSocket to an Ethereum node listening for Swap log events. If a large trade shifts the price, the UI can update the pool price display immediately (within seconds of block finalization). In summary, even in AMMs, the backend indexer and the frontend coordinate to reflect price changes: the indexer produces an authoritative data series, and the frontend supplements with live updates between indexer refreshes. Order-book based DEXes have a heavier burden to keep order books synchronized. In Whales.market’s OTC model, users create offers to buy or sell certain assets. These offers likely exist on-chain (escrowed in a contract with event logs announcing them). The backend indexer captures all new offers and their details (asset, amount, price, seller, etc.), and stores them in a structured form. An order book service in the backend will sort and group these offers (e.g. by asset and price) to build the familiar bid/ask lists. This sorted order book might reside in-memory or in a fast cache (like Redis) for quick access, while the database holds the persistent record. Every time an order is filled or canceled (detected via on-chain events), the indexer updates the database and the in-memory order book. To propagate these changes, the backend likely uses a pub-sub mechanism or event emitters. For example, dYdX’s indexer separates on-chain and off-chain data: on-chain fills go to Postgres, whereas off-chain order book updates are kept in a Redis cache for low-latency access
docs.dydx.exchange
. In their system, a service called Vulcan handles off-chain orderbook ingestion, updating Redis with new or canceled orders and only writing to Postgres for long-term records
docs.dydx.exchange
. This is a common pattern: use an in-memory store for the live order book (since it changes frequently), and a database for archival and backup. To keep the frontend synchronized, the backend often employs WebSockets broadcasting. Each time the order book updates (new order, order filled, etc.), a message is sent to all clients viewing that market. dYdX’s indexer, for example, emits WebSocket events for “any orderbook events that need to be emitted” as it processes each block
docs.dydx.exchange
. A dedicated WebSocket service (called Socks in dYdX v4) consumes these internal events and relays them to connected frontend clients
docs.dydx.exchange
. In practice, Whales.market’s frontend might subscribe to an API channel for a particular asset’s order book. The backend will push updates like “Bid order #123 filled at price X” or “New ask order posted at price Y” to the browser, which then updates the displayed order list in real-time. Price data in an order-book model is essentially derived from the best bid/ask offers or last trade executed. The backend can compute the “last traded price” whenever an order fills and broadcast that as well. All of this ensures that both backend and frontend share a single source of truth for the market state. The backend maintains it (via the indexer and database) and the frontend mirrors it, either by frequent querying or, more efficiently, by reacting to push updates.
Full Stack Integration: Frontend ↔ Backend API ↔ Indexer ↔ Blockchain
Bringing these pieces together, a clear picture of the DEX backend emerges. The frontend (often a web app) interacts with two main things: the blockchain (via user’s wallet or direct RPC calls) and the DEX’s backend API. The backend API is the layer that serves indexed data from the database and handles any additional business logic off-chain. It can be a RESTful API, GraphQL endpoint (as in The Graph’s case), or even custom endpoints. When a user loads a DEX app, the frontend will typically call the backend API for things like “Give me all open buy offers for token XYZ”, “What’s the 24h volume on this market?”, or “List my past trades”. The backend API in turn queries the database that has been filled by the indexer, and returns the results in a convenient format (JSON, etc.). This division of labor allows complex queries to be answered quickly without burdening the blockchain. For instance, computing a user’s trade history might involve aggregating many events – the indexer/DB has done that work already, so the API just fetches from a trades table. Meanwhile, write operations (actual trades, order placements, cancellations) are done by the user directly on-chain through their wallet, since the DEX is non-custodial. The frontend helps construct the transaction and broadcasts it, then relies on the backend (and blockchain itself) to update the state as discussed. Architecture of a modern DEX backend (inspired by dYdX v4). On-chain events flow from the blockchain node into an indexing service, which writes state to a database (Postgres) and cache (Redis). A backend API layer (REST/GraphQL) reads from these to serve frontend requests, while a WebSocket service pushes real-time updates (e.g. order book changes, new trades) to clients
docs.dydx.exchange
docs.dydx.exchange
. To coordinate all these moving parts, DEX backends often utilize pub-sub messaging and caching internally. A message queue (like Kafka or RabbitMQ) or a lightweight pub-sub (like Redis Pub/Sub) connects the indexer components to the API and notification components. In the dYdX architecture, for example, they use Kafka topics to stream events between microservices: one topic carries raw block events to the indexer ingestion service, and another carries prepared updates to the WebSocket service
docs.dydx.exchange
docs.dydx.exchange
. This decoupling ensures that as soon as a new block is processed and an order book update is available, a message is published and the WS service can send it out to users without delay. Similarly, if the DEX backend is running on multiple servers (to handle high load), a pub-sub mechanism helps broadcast state changes to all server instances. Many projects use Redis for this – e.g., when a new trade is indexed, one server publishes a Redis message “trade executed in market X”, and all web servers (and their WebSocket connections) get the memo to send updates to connected clients. Redis also commonly holds ephemeral data like currently open WebSocket connections, caching of frequently accessed info (token metadata, exchange rates), or session data for user-specific settings. The frontend-backend interplay can be summarized as follows: The frontend uses the backend API for aggregated data (because the backend’s database is the easiest way to get things like all orders or historical stats), and uses direct blockchain interaction for executing transactions. The backend indexer silently works in the background to keep the database and caches in sync with the blockchain state. Whenever something notable happens on-chain (an order filled, a liquidity pool updated, etc.), the indexer records it and triggers any necessary notifications. The WebSocket or realtime channel from backend to frontend ensures that users see updates nearly instantaneously, without needing to refresh or continuously poll. This is crucial in fast-moving markets. In real-world implementations, these components are often clearly delineated. Uniswap’s public architecture, for instance, heavily leans on The Graph (decentralized indexer) as its “backend” – the Uniswap frontend directly queries The Graph’s hosted APIs
docs.uniswap.org
 and uses Infura/Alchemy for sending transactions. It may not run its own centralized backend for the exchange logic; instead, the combination of subgraphs + Ethereum node + a bit of client-side computation suffices. On the other hand, Whales.market, providing a more complex multi-chain service (with cross-chain settlement, user dashboards, etc.), likely runs a dedicated backend service. This would include their own indexer nodes for each chain (possibly using SubQuery or custom code), a unified database (e.g. PostgreSQL) that aggregates data from all chains, and an API server that their frontend (web app or even a Telegram bot as hinted by “Whales Intern”) can query. They also have features like a notification system and wallet linking
docs.whales.market
, which suggests a user account layer – likely implemented via an off-chain database of user profiles mapped to wallet addresses. Such a system would use the indexer to watch those addresses and send notifications (via email, Telegram, etc.) when relevant events occur (e.g. “your order was filled”). These are all off-chain services integrated with the core indexing pipeline.
Conclusion
Decentralized exchanges achieve high performance and usability by marrying on-chain smart contracts with sophisticated off-chain infrastructure. Indexers continuously sync blockchain data to off-chain databases, enabling fast queries for order books, trade history, and price analytics
subquery.medium.com
thegraph.com
. RPC nodes and subscription mechanisms feed the indexer with real-time events, while the backend API and WebSocket services ensure that frontends and users receive up-to-the-second updates on transaction statuses, order fills, and price changes
docs.dydx.exchange
ethereum.stackexchange.com
. Through examples like Uniswap’s use of The Graph (subgraphs) and Whales.market’s likely multi-chain indexer setup, we see common patterns: a Postgres (or similar) database for persistent state, caching layers (Redis) for live data, and pub-sub or messaging queues (Kafka/Redis) to propagate events within the system
docs.dydx.exchange
docs.dydx.exchange
. The frontend, backend, and indexer work in concert – the indexer provides the data backbone, the backend API/caches organize and distribute that data, and the frontend renders it and handles user interactions. This layered architecture allows DEX users to enjoy a smooth, near real-time trading experience (rich market data, instantaneous feedback on orders) while preserving the trustless and non-custodial nature of on-chain execution. By examining concrete implementations from leading DEXes, it’s clear that decentralized exchanges heavily rely on these backend infrastructures to bridge the gap between slow, secure blockchain transactions and the fast-paced demands of modern trading platforms. The result is a system where on-chain and off-chain components are tightly integrated – bringing together the immutability of blockchain with the performance of web2 systems – to deliver decentralized trading at scale. Sources: The information above is drawn from public documentation and engineering blogs, including Uniswap’s developer docs and The Graph’s case study on Uniswap
docs.uniswap.org
thegraph.com
, a deep-dive into blockchain indexing techniques
rocknblock.io
, and dYdX’s v4 architecture documentation which illustrates a state-of-the-art indexer design with Postgres, Redis, and Kafka for a DEX
docs.dydx.exchange
docs.dydx.exchange
. Additionally, community Q&A and guides provide insight into transaction monitoring and real-time event handling on Ethereum
ethereum.stackexchange.com
. These examples underscore how DEX backends are built in practice to achieve both decentralization and high performance.
